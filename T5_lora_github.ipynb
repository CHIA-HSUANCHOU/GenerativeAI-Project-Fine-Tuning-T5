{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## 安裝"
      ],
      "metadata": {
        "id": "8o_GSSmm2jTJ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aveewppkxUPo"
      },
      "outputs": [],
      "source": [
        "!pip install transformers datasets evaluate scikit-learn"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install peft rouge_score bert_score"
      ],
      "metadata": {
        "id": "e0MZVm0j2fh5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U bitsandbytes"
      ],
      "metadata": {
        "id": "fRTkzP_ERhcW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "from datasets import Dataset, DatasetDict\n",
        "from sklearn.model_selection import train_test_split\n",
        "from transformers import (\n",
        "    AutoTokenizer, AutoModelForSeq2SeqLM,\n",
        "    T5ForConditionalGeneration, Seq2SeqTrainer, Seq2SeqTrainingArguments,\n",
        "    DataCollatorForSeq2Seq, EarlyStoppingCallback\n",
        ")\n",
        "from peft import get_peft_model, LoraConfig, TaskType, PeftModel\n",
        "import random\n",
        "import numpy as np\n",
        "import torch\n",
        "import os\n",
        "import gc\n",
        "import torch\n",
        "import evaluate\n",
        "\n",
        "seed = 2024\n",
        "\n",
        "random.seed(seed)\n",
        "np.random.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "torch.cuda.manual_seed_all(seed)"
      ],
      "metadata": {
        "id": "PyaQlg1Rxcvl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\""
      ],
      "metadata": {
        "id": "AE-nJ4QREEzi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DMOUmQxv263d",
        "outputId": "d97c7b2b-7bc1-45cc-87ac-3306f02f1518"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_path = \"/content/drive/MyDrive/data/train.json\"\n",
        "test_path = \"/content/drive/MyDrive/data/test.json\""
      ],
      "metadata": {
        "id": "xQHUTeUI27d7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## T5_lora"
      ],
      "metadata": {
        "id": "GybxXQvc3sAC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# === 參數設定 ===\n",
        "model_name = \"google/flan-t5-xl\"\n",
        "\n",
        "max_input_length = 3096\n",
        "max_target_length = 600\n",
        "\n",
        "# === 載入並切分資料 ===\n",
        "with open(train_path, \"r\", encoding=\"utf-8\") as f:\n",
        "    data = [json.loads(line) for line in f]\n",
        "\n",
        "train_data, val_data = train_test_split(data, test_size=0.1, random_state=42)\n",
        "train_ds = Dataset.from_list(train_data)\n",
        "val_ds = Dataset.from_list(val_data)\n",
        "dataset = DatasetDict({\"train\": train_ds, \"validation\": val_ds})"
      ],
      "metadata": {
        "id": "9A-lZRXUDdAC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# === 載入 tokenizer 與模型 ===\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
        "model.gradient_checkpointing_enable()\n",
        "\n",
        "# === 預處理 ===\n",
        "def preprocess(example):\n",
        "    prompt = (\n",
        "    \"You are a professional academic summarizer. \"\n",
        "    \"Write a precise and objective abstract for the following research introduction. \"\n",
        "    \"Do not include poetic or exaggerated language. \"\n",
        "    \"Only describe the main objectives, methods, and key findings of the paper. \"\n",
        "    \"If the text contains formulas, mathematical notations, or specific numerical results, retain them in the abstract. \"\n",
        "    \"Do not add personal opinions or restate this prompt. Use a formal academic tone.\\n\\n\"\n",
        "    \"Introduction: \"\n",
        "    )\n",
        "    inputs = prompt + example[\"introduction\"]\n",
        "    targets = example[\"abstract\"]\n",
        "    model_inputs = tokenizer(inputs, max_length=max_input_length, truncation=True, padding=\"max_length\")\n",
        "    with tokenizer.as_target_tokenizer():\n",
        "        labels = tokenizer(targets, max_length=max_target_length, truncation=True, padding=\"max_length\")[\"input_ids\"]\n",
        "        labels = [t if t != tokenizer.pad_token_id else -100 for t in labels]\n",
        "    model_inputs[\"labels\"] = labels\n",
        "    return model_inputs\n",
        "\n",
        "tokenized_dataset = dataset.map(preprocess, batched=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 168,
          "referenced_widgets": [
            "4541e82564d14dcb8d1905640207d0c5",
            "27d5f3b45eaf476c91b5207196d6b79a",
            "7f9c32d64c1a4f259e8d6b2662d77f9f",
            "5f391e5fd396460e9458132004485ac9",
            "affed29cfde841d5b1b6788208d92754",
            "6d40263a5f14457da9eef99bdf636baa",
            "c018cd9a0ef5494a93a3d999a1ee8fe8",
            "b9f87601db604f9f9557da92cc22b557",
            "af5a4f3318394964827d670b17ca0d7c",
            "386459e534d4472a88682ea9a7c17e79",
            "3e750626cb8049029308e370986f8d7e",
            "1ffa768a17c4441abbdced4e16e90a84",
            "b854ad44f328435782bd9130e1cbb398",
            "8af735fd9ebf4f90afdcaa2bc2858b9e",
            "b82e03c112cd4e32b8d458b66a54a7cf",
            "0d52966a980a4dddbb6490f737e58528",
            "4f90c1c7a9fa4350b3e09738d536a43a",
            "fd9e0f64de5641d5a38130faa8c0dd26",
            "c4e06f88f10349e2b49c839b3a12af95",
            "c5224d153882406593ffd0a02766efa1",
            "cb1ea22cdd704487969c46238d190d82",
            "335fa2dc484b4eecb02d07f6c1299c58",
            "e1d0873fafe6440dacb7c3dee61658c0",
            "d882a8e0704d460692379f281c1759bc",
            "e445f0dc051d48ae904d61b01d3da1ec",
            "6feafe5ddceb4bd8936acd99d528bc52",
            "814836cfeaa847eda89eb30962b33a0e",
            "868a7e3232384ff8bf21be34e1ea699b",
            "fed36fee5e0341848e86a81a2298325d",
            "44098ffcf5cb40d89c7b163d78bbf7c4",
            "bbd78565fd12485e97aefa628c46fef8",
            "e7048806ff164f378956755b532a023f",
            "08702513391a478f819fc749b20879fd"
          ]
        },
        "outputId": "c44310af-d4ff-4204-bc6b-5bdacb3ebe38",
        "id": "1jEHEuGKDdAC"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "4541e82564d14dcb8d1905640207d0c5"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/367 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "1ffa768a17c4441abbdced4e16e90a84"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/transformers/tokenization_utils_base.py:3980: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/41 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e1d0873fafe6440dacb7c3dee61658c0"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(tokenizer.decode(tokenized_dataset[\"train\"][0][\"input_ids\"], skip_special_tokens=True))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0c5eeb85-e922-41a1-c7c6-ff7358ef9c93",
        "id": "9AjTXAmYDdAD"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "You are a professional academic summarizer. Write a precise and objective abstract for the following research introduction. Do not include poetic or exaggerated language. Only describe the main objectives, methods, and key findings of the paper. If the text contains formulas, mathematical notations, or specific numerical results, retain them in the abstract. Do not add personal opinions or restate this prompt. Use a formal academic tone. Introduction: With the rapid development of deep vision detection technology in artificial intelligence, detecting anomalies/defects on the surface of industrial products has received unprecedented attention.Changeover in manufacturing refers to converting a line or machine from processing one product to another.Since the equipment has not been completely fine-tuned after the start of the production line, changeover frequently results in unsatisfactory anomaly detection (AD) performance.How to achieve rapid training of industrial product models in the changeover scenario while assuring accurate anomaly detection is a critical issue in the actual production process.The current state of AD in the industry is as follows: (1) In terms of detection accuracy, the performance of state-ofthe-art (SOTA) AD models degrades dramatically during the changeover.Current mainstream work utilizes a considerable amount of training data as input to train the model, as shown in Fig. 1(a).However, this will make data collecting challenging, even for unsupervised learning.As a result, many approaches based on few-shot learning at the price of accuracy have been proposed.For instance, Huang et al. (2022) employ meta-learning, as shown in Fig. 1(b).While due to complicated settings, it is impossible to migrate to the new product during the changeover flexibly, and the detection accuracy cannot be guaranteed.(2) In terms of training speed, when a large amount of data is utilized for training, the training progress for new goods is slowed in the actual production line.As is well-known, vanilla unsupervised AD requires to collect a large amount of information.Even though meta-learning works in few-shot learning, as shown in Fig. 1(b), it is still necessary to train a massive portion of previously collected data.For our setting (c), there is no requirement to aggregate training categories in advance.The proposed model, vision isometric invariant GNN, can fast obtain the invariant feature within a few normal samples, and its accuracy outperforms models trained in a meta-learning context.We state that AD of industrial products requires just a small quantity of data to achieve performance comparable to a large amount of data, i.e., a small quantity of image data can contain sufficient information to represent a large number of data.Due to the fact that industrial products are manufactured with high stability (no evident distortion of shape and color cast), the taken images lack the diversity of natural images, and there is a problem with the shooting angle or rotation.Therefore, it is essential to extract rotation-invariant structural features.As graph neural networks (GNNs) are capable of robustly extracting non-serialized structural features (Han et al. (2022), Bruna et al. (2013), Hamilton et al. (2017), Xu et al. (2018)), and they integrate global information better and faster Wang et al. (2020); Li et al. (2020).They are more suited than convolution neural networks (CNNs) to handle the problem of extracting rotation-invariant features.For this reason, the core idea of the proposed GraphCore method in this paper is to use the visual isometric invariant features (VIIFs) as the anomaly measurement features.In the method using memory bank (M) as the AD paradigm, PatchCore (Roth et al. (2022)) uses ResNet (He et al. ( 2016)) as the feature extractor.However, since their features obtained by CNNs do not have rotation invariance (Dieleman et al. ( 2016)), a large number of redundant features are stored in M. Note that these redundant features maybe come from multiple rotation features of the same patch structure.It will hence require a huge quantity of training data to ensure the high accuracy of the test set.To avoid these redundant features, we propose VIIFs, which not only produce more robust visual features but also dramatically lower the size of M and accelerate detection.Based on the previous considerations, the goal of our work is to handle the cold start of the production line during the changeover.As shown in Fig. 1(c), a new FSAD method, called GraphCore, is developed that employs a small number of normal samples to accomplish fast training and competitive AD accuracy performance of the new product.On the one hand, by utilizing a small amount of data, we would rapidly train and accelerate the speed of anomaly inference.On the other hand, because we directly train new product samples, adaptation and migration of anomalies from the old product to the new product do not occur.Contributions.In summary, the main contributions of this work are as follows:• We present a feature-augmented method for FSAD in order to investigate the property of visual features generated by CNNs.• We propose a novel anomaly detection model, GraphCore, to add a new VIIF into the memory bank-based AD paradigm, which can drastically reduce the quantity of redundant visual features.• The experimental results show that the proposed VIIFs are effective and can significantly enhance the FSAD performance on MVTec AD and MPDD.Related Work.Few-shot anomaly detection (FSAD) is an attractive research topic.However, there are only a few papers devoted to the industrial image FSAD.Some works (Liznerski et al. (2020); Pang et al. (2021); Ding et al. (2022)) experiment with few-shot abnormal images in the test set, which contradicts our assumptions that no abnormal images existed.While others (Wu et al. (2021); Huang et al. (2022)) conduct experiments in a meta-learning setting.This configuration has the disadvantage of requiring a high number of base class images and being incapable of addressing the shortage of data under cold-start conditions in industrial applications.PatchCore (Roth et al. (2022)), SPADE (Cohen & Hoshen (2020)), and PaDiM (Defard et al. (2021)) investigated AD performance on MVTec AD in a few-shot setting.However, these approaches are not intended for changeover-based few-shot settings.Thus their performance cannot satisfy the requirements of manufacturing changeover.In this research, we propose a feature augmentation method for FSAD that can rapidly finish the training of anomaly detection models with a small quantity of data and meet manufacturing changeover requirements.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# === LORA ===\n",
        "lora_config = LoraConfig(\n",
        "    r=16,\n",
        "    lora_alpha=32,\n",
        "    target_modules=[\"q\", \"v\", \"k\", \"o\"],\n",
        "    lora_dropout=0.1,\n",
        "    bias=\"none\",\n",
        "    task_type=TaskType.SEQ_2_SEQ_LM\n",
        ")\n",
        "\n",
        "model = get_peft_model(model, lora_config)\n",
        "model.print_trainable_parameters()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4a3afbc8-c19f-4709-ddd7-a7155f66b46e",
        "id": "q1pe0rq7DdAD"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "trainable params: 18,874,368 || all params: 2,868,631,552 || trainable%: 0.6580\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from functools import reduce\n",
        "from operator import mul\n",
        "\n",
        "total_params = sum(p.numel() for p in model.parameters())\n",
        "total_size = sum(p.numel() * p.element_size() for p in model.parameters()) / (1024 ** 2)\n",
        "\n",
        "print(f\"🧠 Total parameters: {total_params:,}\")\n",
        "print(f\"💾 Approx. model size: {total_size:.2f} MB\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "81e835a2-2fd1-4583-ae81-4ad62c09c637",
        "id": "J8s4t0q7DdAD"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🧠 Total parameters: 2,868,631,552\n",
            "💾 Approx. model size: 10942.96 MB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# === 訓練參數設定 ===\n",
        "training_args = Seq2SeqTrainingArguments(\n",
        "    output_dir=\"./t5_43\",\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    logging_strategy=\"steps\",\n",
        "    logging_steps=10,\n",
        "    per_device_train_batch_size=1,\n",
        "    per_device_eval_batch_size=1,\n",
        "    gradient_accumulation_steps=2,\n",
        "    learning_rate=5e-5,\n",
        "    num_train_epochs=3,\n",
        "    warmup_ratio=0.1,\n",
        "    predict_with_generate=True,\n",
        "    fp16=False,\n",
        "    bf16=True,\n",
        "    save_total_limit=2,\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"eval_loss\",\n",
        "    label_smoothing_factor=0.1,\n",
        "    greater_is_better=False,\n",
        "    optim=\"adafactor\",\n",
        "    lr_scheduler_type=\"cosine\",\n",
        "    report_to=\"none\",\n",
        "    seed=seed\n",
        ")\n",
        "\n",
        "# === 建立 Data Collator（避免 loss 為 nan） ===\n",
        "data_collator = DataCollatorForSeq2Seq(\n",
        "    tokenizer=tokenizer,\n",
        "    model=model,\n",
        "    label_pad_token_id=-100,\n",
        "    padding=True\n",
        ")\n",
        "\n",
        "# === 開始訓練 ===\n",
        "trainer = Seq2SeqTrainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_dataset[\"train\"],\n",
        "    eval_dataset=tokenized_dataset[\"validation\"],\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=data_collator,\n",
        "    callbacks=[EarlyStoppingCallback(early_stopping_patience=3)]\n",
        ")\n",
        "trainer.train()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        },
        "outputId": "c4af3bd9-81af-49fc-f400-00d4637d76f4",
        "id": "YYNVN-XvDdAE"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/transformers/training_args.py:1611: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n",
            "<ipython-input-30-b36549ccf47d>:42: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = Seq2SeqTrainer(\n",
            "No label_names provided for model class `PeftModelForSeq2SeqLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='549' max='549' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [549/549 44:52, Epoch 2/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>3.588100</td>\n",
              "      <td>3.478110</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>3.371500</td>\n",
              "      <td>3.327720</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.48.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=549, training_loss=3.5816610437056187, metrics={'train_runtime': 2697.0451, 'train_samples_per_second': 0.408, 'train_steps_per_second': 0.204, 'total_flos': 5.706367329632256e+16, 'train_loss': 3.5816610437056187, 'epoch': 2.9863760217983653})"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "base_model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
        "model = PeftModel.from_pretrained(base_model, \"/content/t5_43/checkpoint-549\", is_trainable=True)\n",
        "\n",
        "model.print_trainable_parameters()\n",
        "：\n",
        "lora_params = sum(p.numel() for n, p in model.named_parameters() if \"lora_\" in n)\n",
        "print(f\"LoRA 導入參數數量：{lora_params:,}\")\n",
        "\n",
        "model = model.merge_and_unload()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84,
          "referenced_widgets": [
            "b382db85a6ff443a8209633dd35fb0ea",
            "c9c1acfe1cef4b229928d03c5d187013",
            "15861bcc73574ceb8ac89c5a73e4f41c",
            "f1e4b53d212b4ff084ef84e55cbaca92",
            "8859759293a148fc90c749800ce688a4",
            "4b13de2434004cf29d8bc73581f3f223",
            "40057afbb6b54d188f874c67822f66fe",
            "3b4336269fa449a490e8b4fa8bc7e524",
            "0b3bdc21cb4147bdac28e224d74ca8e9",
            "7831221709094bcb86f510c1f08ec24c",
            "66c1b5fbf33c462d87dcc1580c35a80c"
          ]
        },
        "outputId": "ee267983-092e-4d62-e2ca-218b38a5ca0d",
        "id": "UTKmTb5vDdAE"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b382db85a6ff443a8209633dd35fb0ea"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "trainable params: 18,874,368 || all params: 2,868,631,552 || trainable%: 0.6580\n",
            "LoRA 導入參數數量：18,874,368\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(type(model))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "641ed22e-a131-42fa-dac5-7cd5e10f60f9",
        "id": "ikiPyFU6DdAE"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'transformers.models.t5.modeling_t5.T5ForConditionalGeneration'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.save_pretrained(\"t5_43_final\")\n",
        "tokenizer.save_pretrained(\"t5_43_final\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "49d844e2-d500-4376-e35b-c7c59bef848c",
        "id": "NXpOMTdaDdAF"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('t5_43_final/tokenizer_config.json',\n",
              " 't5_43_final/special_tokens_map.json',\n",
              " 't5_43_final/spiece.model',\n",
              " 't5_43_final/added_tokens.json',\n",
              " 't5_43_final/tokenizer.json')"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import login\n",
        "login(\"hfxxxxx\")\n",
        "\n",
        "model.push_to_hub(\"xxx/lora43\")\n",
        "tokenizer.push_to_hub(\"xxx/lora43\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 278,
          "referenced_widgets": [
            "ed57b6c6e8ab4b5ebfa09f76f6054cf8",
            "febedbf05be143a089ab173a777b92c1",
            "c4781ce76ac64975a3a7cf3e6dfee3a7",
            "6768bef121444d0e84c4277ca23fffc2",
            "0a9a0db85bb840218fc041f17fe0a143",
            "a01b14de6d5940abb8b1eb43ae1d8b80",
            "fed76034d4cf4147ad1b31df329c75bf",
            "0dbf93a9cbec402692ce56ad68084ebe",
            "ca4dac933e054ddf9840a3688102ce34",
            "a433e137f9084d9d88f8d433d2dfd424",
            "3b5ad5ee8d9f4ac3ba4c77790a1836bc",
            "e598ab3baec44ff9ba72b068ec008e43",
            "59eabe9f612c433d9599361c5be32ac4",
            "e0d6d60f52aa46d18b62050d67cdc78c",
            "f31ee535e1dd4960a55376e9cf6510d8",
            "ae3c8d22af7549038d4ce49f9e51cd36",
            "bf92d241a2af4cfc9df6539918265d03",
            "9512a97bde0e4bddae44fd3ac1478238",
            "e282aa8af1cd487a8067d0792306e681",
            "c6675d344293449884a445e0777919c6",
            "8031a3b67d8e4e92ad576f02de3bed5d",
            "e4f0e4def6f24c9d958af3c15585c0c7",
            "9b0f066222c54f158c2bfc550ce6a530",
            "5d481bbbbea5496e844be2b65f473b79",
            "249267b0194d4cc1a5ad29df579c9b0b",
            "7ec2e41155a342f6b3f280cb1a2c654c",
            "2bea4b49e16546bea649a096f99a0842",
            "a4979d36ca844f4e97a4a94f1ef6be7d",
            "8c4e4a905c944247b623921477872371",
            "634f5fca82d94cc0ad4a1fc49f1fcdb0",
            "ac88b08654f946d3bc158cd7764a0c0d",
            "f3a0532cc1f24a7a9dec357a71aad055",
            "ba3b3684af24459cbcc021765a6be043",
            "9ebaff9b630f47fdb30a6e928ce46814",
            "213fbca492a84916ba0cbbd1798ee7a1",
            "3b394c2a4d3a43de8352f823351617de",
            "141028ad947343cba809483445169a80",
            "e953968b9a454988b248249104ca13aa",
            "2fd8a554eaf347c5a3d2ba6c08690f86",
            "babb339d62ca4eebbbbd78d05e2b1650",
            "6d8a12a71b264472a11ba61a635cffa1",
            "81921e680078454a8876c486a60c1798",
            "52542b361da34067a52956f276dd0008",
            "69dec34a576541d7bcd3c8c87e8b4d4f",
            "29bd688d89304312817a0fa5d9dbb13b",
            "b4af8a261dc14e1a9db318bb48a746b4",
            "47d4d047f0e24d5fa7e7be5040071e71",
            "44e6ea36c2d741278e4d0407265af629",
            "18fed145d5a747a1b5e55f489ff5d037",
            "3ed37850d6384352bedd496b48ada80e",
            "59378840d51b4b939d4909b433e592d3",
            "3ab7ae61dd864aa289054d64246c9501",
            "8982768aa9bd436ab126896eeb9f3b13",
            "68dafda1bf4f4899b8f5c3976253a5fc",
            "e46a7cd668394a2696ce1b244b8b664f",
            "42cee9efba0b4f819ded076b0b3769c6",
            "8b3e01a952394ceba89c8a42f1b8c2f1",
            "78beb4592ad042afbab601f540e58ee3",
            "328193e770d246119e3d35c9cf1e4a79",
            "371b4104b48a4f079eef822b2391e3aa",
            "06af10c6c70a436793f192679f4bbc0c",
            "7d677c3e49fa4fa5a89a2eb6949080f8",
            "995dd35dd0334c3eafd5c1e8ec0d1684",
            "2f742831a8ec48a0bb18583a04117117",
            "92c2ac0bb11a4bd5bb8e36b85ebcb149",
            "a273ccdc006d4ea587652f9cc0e40c52"
          ]
        },
        "outputId": "3d23b300-0a96-483e-c3f5-212d2265ee9b",
        "id": "DPMnd9Q_DdAF"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model-00002-of-00003.safetensors:   0%|          | 0.00/4.97G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ed57b6c6e8ab4b5ebfa09f76f6054cf8"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model-00001-of-00003.safetensors:   0%|          | 0.00/4.99G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e598ab3baec44ff9ba72b068ec008e43"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Upload 3 LFS files:   0%|          | 0/3 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9b0f066222c54f158c2bfc550ce6a530"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model-00003-of-00003.safetensors:   0%|          | 0.00/1.43G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9ebaff9b630f47fdb30a6e928ce46814"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "README.md:   0%|          | 0.00/5.17k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "29bd688d89304312817a0fa5d9dbb13b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "42cee9efba0b4f819ded076b0b3769c6"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "CommitInfo(commit_url='https://huggingface.co/NCCUTAT/T5_lora43/commit/8eee9d2e404682cf17df37570132bb75875d0eac', commit_message='Upload tokenizer', commit_description='', oid='8eee9d2e404682cf17df37570132bb75875d0eac', pr_url=None, repo_url=RepoUrl('https://huggingface.co/NCCUTAT/T5_lora43', endpoint='https://huggingface.co', repo_type='model', repo_id='NCCUTAT/T5_lora43'), pr_revision=None, pr_num=None)"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "del model\n",
        "del tokenizer\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "6wrDiQ64DdAF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 驗證"
      ],
      "metadata": {
        "id": "MR794j13DdAF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "from huggingface_hub import login\n",
        "login(\"hfxxxxx\")\n",
        "\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(\"xxx/lora43\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"xxx/lora43\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 401,
          "referenced_widgets": [
            "fe0ae6b3f45f4301a4e6024193e84cdd",
            "b988f21e01c74af9892f4ce2cf4357f8",
            "280f20ac3e7c4c0c913afaa7f0c590f4",
            "23d42abbb3fc4db1a2c319013abb53c8",
            "4155eb98cb1d4d079fcdc8c045cfa11f",
            "14665ec3541544d5b9d6db10cbc6d4b0",
            "e949f1cfb34f4ef59aa0bfd76e268162",
            "057121c52a564129808f3b552a4b6436",
            "a010bad0966f4ed9bf3bb1a30dc5117c",
            "1476614d548f424a822e8d5d67d25992",
            "6d7e619a30234469a14b686b5609e2ab",
            "ae86efc8cfa142eca6e7cb22c9a2a5bf",
            "9998262b271342ad93bcae0e09ced52e",
            "9b878cec04a74a3eb3d0a984de768880",
            "e2598528401f4fb4af0110b57bf6e200",
            "830cacbc2f014dcb8c8216a6665f1c89",
            "f8370df083044e329446a560405856da",
            "2f34869ce37d4ff8828261c75e9be3d0",
            "426f6fbac78d4cd3beebb26bd61f9a85",
            "775eb99be3064026a6db5ed38376dd84",
            "3b1ac0ff174241c7878009dd45c23cdb",
            "9d8cd63f03f24d7d88bb24fb119953be",
            "ccfc0e2c21154da7887e147a8ced7e10",
            "49753384f2b5438094818daca1e91509",
            "1d198c9014484ea39964e106fc04dc1a",
            "97871caa69614047a993be43f5ce16ad",
            "dbd4dcd49e6144869437cec794cc20be",
            "4c9043860dee48cd913be4ea18a6cda8",
            "e31ec50c44f644cba84990b674103e5e",
            "2333ee7e00c745d298fcbd475e509b97",
            "787d2f9450af4c26b1e382d090014996",
            "e9c518c6a6344ad89378b75fca55c247",
            "411252eada554ba2abd931e1b4599533",
            "b14ce4973622475198194b6dd4c90841",
            "38ae57987aa749468f01f3010f4275b9",
            "6ef35497467c4a5190c69c1a7d2989e0",
            "fe0e30bb06da425c9ee532393ad9060c",
            "8fb9609ef23f49328ee7d278ea99f490",
            "641933f6c31b4b3abf4af4c14854eb08",
            "4a7864c1439747b6929d9fba365f85fa",
            "a614809ed0554568bb01b043e7492880",
            "834b221a97e14f47b0d41bd92a8bea50",
            "3cb5669f1a1c46429bd2480d2c9f030d",
            "49b16058df054e7f8853be32bdcfd2d1",
            "51440e6164ea4b1480173f377f2adbf0",
            "45ab5b264f25460cb63b29b1e809c026",
            "ae8c9217d43c4e6d836de0776a73ab20",
            "3060a0b789ea440ebbedad2e97b05b4a",
            "e3d767a1ba794921959b6b9ece618bad",
            "005d3ad6dd3a4aeeabdf9a1e20bc1992",
            "a1a3fae9f6164c91b0698e76fa14d404",
            "25fd83a1947f4c7fba031e81bccf0d89",
            "a47d17c95f44483db2abacd073826586",
            "86e4a9e49a1c43809217ccc12bcd1101",
            "98a991765c194b9bb80c7b1245c462dd",
            "29bfbc59ce5f4fb694360cf1f3000cbc",
            "a09544a9926046b58b87c57fc5721a5c",
            "360e8aea00954ffa86699d962e599860",
            "58423c58306f433c8f5a234e9749d35d",
            "7373e0f1f4b144629f2751c6ac67f8f6",
            "359437f28ae04bb091c0e87ca39416be",
            "c2b8a16050244841b811aff932e15e21",
            "950166bf999846cd95ea246ebaa86739",
            "f85cf2252c224983b31efc0fee754de0",
            "a3f5b70a249d45668cc9a77e261e17a9",
            "38824f2e2d10404fbf83f2672d0429bc",
            "b2ea76d0029944178967f56f1768b683",
            "f6a0b93b2f8f4cd3a6f535634d8bd157",
            "7569b1282e2a44f386d34252eb1b0ffb",
            "c1d1e205aafd4470b320f2283731d76b",
            "cf6a485a2619470ca99ce8ef186740d6",
            "e7216cf145de4681963dcb82edcc4d45",
            "088ebe4477374c10bfddd4ae1eef9fd7",
            "8bf8e0a3b983494bba201cbfb1fc56b3",
            "2d408d7e87b34f5ea6cd93490e1f83bd",
            "cc32bdc5124b4fc99b97cdb37a898f3e",
            "647ec6557bd04ec393e02fb82a85e21a",
            "f9b7c08098b644148b2bc034160dcf35",
            "7350b7944d7f4a5fad2c9b447841af8b",
            "e97fd44a04c744bf995a443d4fcb0862",
            "7f88022d58f24aa8a9b02fab2aacadea",
            "7f9c222ed40c4b25b67a93cd86c25413",
            "bad3f63a8699423b9d1caa25b9c35e1b",
            "0bdfd175e44e479db945a1d264153d56",
            "e1833265dc954fb19934c28738d06e03",
            "d68bb9be98d046d3bdc67b4a8fb4bb7e",
            "7b8351ee7e6b495b9534704c338a67c1",
            "7984409bb14c4cb09cddac8d007ce7cc",
            "3d9bfefdfdcd406ca80df86d05b5dd98",
            "d1651a2cc37f4fa999b0e4d2a95a81b1",
            "ed4de37cf23a4fd78bd3af20043ca093",
            "3aacc4f4b5044e30bc04ba708c12568a",
            "a35fe071e51848cdaf1ec8cde1016111",
            "2e225a72244e47a5a4662b2de6d2423e",
            "c0c87e83fcfa49478dbc0ef87f5a7293",
            "d8bd4e1f371f48eebaf37602139e5a2d",
            "963a50d7f4ab4ded94d545c09dce43a7",
            "caad93e70d1b419e8a079b6e8e5c13e2",
            "6aa9669822ba442ab95a927d6f574a75",
            "1a206d6061574686a77e43be0a56a5ee",
            "e14766e808944b38a89c0a39050fa663",
            "714b21d53c4d4708b22c7fb0a75dc05a",
            "958211bf7dd64582aa8e2f6e9bfe58c1",
            "3a902e75e60d4f5ab0ccf376b70422b3",
            "96aafb4fec384c059ac990f5ee3f347a",
            "9234e37d36804d41b51e066646040d05",
            "191d7217a167471d9ef4f9127f510e1e",
            "66d94f96837c4d10b99370d508ba7ac1",
            "b3692d58c23c4f2e93b916b959143521",
            "bbaa1b6a314743a3a577d01c6f89806b",
            "64b66879444e429491442af9996a24b0",
            "2d27b9d7537e47e7a1190302b88b86d8",
            "04efbdd497c44591b5362cf937fe2cfe",
            "81778085cc144de59902c67162f684dd",
            "93188decbdbb47a59b18d975bb5f9bce",
            "d8e48c948f8f49ff938dbbe5454b2720",
            "dbf604673cf546f889259832c871cb4a",
            "07d6cb3de4d74879bd889ed4f4f75036",
            "9ac6f599576d4399b4abdf901044dbff",
            "9f33e2a227a34e2a86d7fe4d1daf7137",
            "371873dce2ac435e97fabba2ebfa0b8d",
            "ab21279b7e38481487612cea4e1d269b",
            "48fffcf38a354020ae91e240fcb78765",
            "b9890b06e1e74c23837783815fac836b",
            "68086c4a3727469583cb5d240bb4698c",
            "52aeb1c56ef7410bba6191cd3a465e69",
            "10aa66f7cc6b481d8b5c690c41d9fb73",
            "c34dfc5357c64b4f80f5f6a22fdceb17",
            "09ff5b87b84f4c479e585194b259712c",
            "859ae81ff1ea438bbe697b254966fa8c",
            "a76c71d1691a4c728a60300feb58d7f6",
            "9838a974d477424fbf53ef047a20d271"
          ]
        },
        "outputId": "abbcb76f-dd98-4053-c4d9-a8b547c5fcf6",
        "id": "2szJHZD0DdAF"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/1.52k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "fe0ae6b3f45f4301a4e6024193e84cdd"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors.index.json:   0%|          | 0.00/50.6k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ae86efc8cfa142eca6e7cb22c9a2a5bf"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Fetching 3 files:   0%|          | 0/3 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ccfc0e2c21154da7887e147a8ced7e10"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model-00001-of-00003.safetensors:   0%|          | 0.00/4.99G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b14ce4973622475198194b6dd4c90841"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model-00003-of-00003.safetensors:   0%|          | 0.00/1.43G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "51440e6164ea4b1480173f377f2adbf0"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model-00002-of-00003.safetensors:   0%|          | 0.00/4.97G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "29bfbc59ce5f4fb694360cf1f3000cbc"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b2ea76d0029944178967f56f1768b683"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/142 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f9b7c08098b644148b2bc034160dcf35"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/21.0k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "3d9bfefdfdcd406ca80df86d05b5dd98"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "1a206d6061574686a77e43be0a56a5ee"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/2.42M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "64b66879444e429491442af9996a24b0"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/2.54k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ab21279b7e38481487612cea4e1d269b"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = AutoModelForSeq2SeqLM.from_pretrained(\"/content/t5_43_final\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"/content/t5_43_final\")"
      ],
      "metadata": {
        "id": "wf_6-r1GDdAG",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "039727e31d2b4e05b70a085dc16df2dd",
            "095825bc5a9b431481c47267ec2ed935",
            "811ee7389e044e2ca2c3d4a748562a2a",
            "2d91ff5b385a4da7abe4bfc6d13e3666",
            "66ad6fd8bf384b428ae8652b44e13de9",
            "d958e519fcfd431e9b20abf30ac12627",
            "751bc48fcdc94f15af8daa6fc8586f8e",
            "efb5fe5f32784406808bc84e61a9c2d5",
            "152222ed719049aa8e48bd37b0141d51",
            "e31df03a72804fe3b83f55b1b7553178",
            "bce2d987e8cb482290f4d97142e0f2f8"
          ]
        },
        "outputId": "8786fd68-8894-469c-a13d-40a8f80a4b0f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "039727e31d2b4e05b70a085dc16df2dd"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from functools import reduce\n",
        "from operator import mul\n",
        "\n",
        "total_params = sum(p.numel() for p in model.parameters())\n",
        "total_size = sum(p.numel() * p.element_size() for p in model.parameters()) / (1024 ** 2)\n",
        "\n",
        "print(f\"Total parameters: {total_params:,}\")\n",
        "print(f\"Approx. model size: {total_size:.2f} MB\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cbc879df-2176-4875-a1a3-9d7cfbae647f",
        "id": "rGvCh9kEDdAG"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total parameters: 2,849,757,184\n",
            "Approx. model size: 10870.96 MB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# === 推理函式 ===\n",
        "def generate_summary(text):\n",
        "    prompt = (\n",
        "    \"You are a professional academic summarizer. \"\n",
        "    \"Write a precise and objective abstract for the following research introduction. \"\n",
        "    \"Do not include poetic or exaggerated language. \"\n",
        "    \"Only describe the main objectives, methods, and key findings of the paper. \"\n",
        "    \"If the text contains formulas, mathematical notations, or specific numerical results, retain them in the abstract. \"\n",
        "    \"Do not add personal opinions or restate this prompt. Use a formal academic tone.\\n\\n\"\n",
        "    \"Introduction: \"\n",
        "    )\n",
        "    input_text = prompt + text\n",
        "    inputs = tokenizer(input_text, return_tensors=\"pt\", truncation=True, max_length=max_input_length).to(model.device)\n",
        "    outputs = model.generate(\n",
        "    **inputs,\n",
        "    max_new_tokens=max_target_length,\n",
        "    min_length=200,\n",
        "    num_beams=4,\n",
        "    early_stopping=True,\n",
        "    repetition_penalty=1.2,\n",
        "    no_repeat_ngram_size=3,\n",
        ")\n",
        "    return tokenizer.decode(outputs[0], skip_special_tokens=True)"
      ],
      "metadata": {
        "id": "w1lO-xC-DdAG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "print(\"\\n=== 驗證集摘要預測（前 2 筆） ===\\n\")\n",
        "predictions = []\n",
        "references = []\n",
        "prompts = []\n",
        "\n",
        "for sample in tqdm(dataset[\"validation\"].select(range(4)), desc=\"Generating summaries\"):\n",
        "    article = sample[\"introduction\"]\n",
        "    gt_abstract = sample.get(\"abstract\", \"\")\n",
        "\n",
        "    prompt = (\n",
        "    \"You are a professional academic summarizer. \"\n",
        "    \"Write a precise and objective abstract for the following research introduction. \"\n",
        "    \"Do not include poetic or exaggerated language. \"\n",
        "    \"Only describe the main objectives, methods, and key findings of the paper. \"\n",
        "    \"If the text contains formulas, mathematical notations, or specific numerical results, retain them in the abstract. \"\n",
        "    \"Do not add personal opinions or restate this prompt. Use a formal academic tone.\\n\\n\"\n",
        "    \"Introduction: \" + article\n",
        "    )\n",
        "\n",
        "    summary = generate_summary(article)\n",
        "    predictions.append(summary.strip())\n",
        "    prompts.append(prompt.strip())\n",
        "    references.append(gt_abstract.strip())\n",
        "\n",
        "for i in range(2):\n",
        "    print(\"------------------------------------------------------\")\n",
        "    print(f\"[Sample {i + 1}]\")\n",
        "    print(\"\\n▶ Prompt：\\n\", prompts[i])\n",
        "    print(\"\\n▶ Ground Truth 摘要：\\n\", references[i])\n",
        "    print(\"\\n▶ 模型生成摘要：\\n\", predictions[i])\n",
        "    print(\"------------------------------------------------------\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ad6dfed0-3064-4ff1-e76c-407c7a138283",
        "id": "TiiE0CNoDdAG"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== 驗證集摘要預測（前 3 筆） ===\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Generating summaries: 100%|██████████| 4/4 [12:19<00:00, 184.87s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "------------------------------------------------------\n",
            "[Sample 1]\n",
            "\n",
            "▶ Prompt：\n",
            " You are a professional academic summarizer. Write a precise and objective abstract for the following research introduction. Do not include poetic or exaggerated language. Only describe the main objectives, methods, and key findings of the paper. If the text contains formulas, mathematical notations, or specific numerical results, retain them in the abstract. Do not add personal opinions or restate this prompt. Use a formal academic tone.\n",
            "\n",
            "Introduction: The channel configuration (a.k.a.. filter numbers or channel numbers) of a neural network plays a critical role in its affordability on resource constrained platforms, such as mobile phones, wearables and Internet of Things (IoT) devices.The most common constraints (Liu et al., 2017b;Huang et al., 2017;Wang et al., 2017;Han et al., 2015a), i.e., latency, FLOPs and runtime memory footprint, are all bound to the number of channels.For example, in a single convolution or fully-connected layer, the FLOPs (number of Multiply-Adds) increases linearly by the output channels.The memory footprint can also be reduced (Sandler et al., 2018) by reducing the number of channels in bottleneck convolutions for most vision applications (Sandler et al., 2018;Howard et al., 2017;Ma et al., 2018;Zhang et al., 2017b).Despite its importance, the number of channels has been chosen mostly based on heuristics.LeNet-5 (LeCun et al., 1998) selected 6 channels in its first convolution layer, which is then projected to 16 channels after sub-sampling.AlexNet (Krizhevsky et al., 2012) adopted five convolutions with channels equal to 96, 256, 384, 384 and 256.A commonly used heuristic, the \"half size, double channel\" rule, was introduced in VGG nets (Simonyan & Zisserman, 2014), if not earlier.The rule is that when spatial size of feature map is halved, the number of filters is doubled.This heuristic has been more-or-less used in followup network architecture designs including ResNets (He et al., 2016;Xie et al., 2017), Inception nets (Szegedy et al., 2015;2016;2017), MobileNets (Sandler et al., 2018;Howard et al., 2017) and networks for many vision applications.Other heuristics have also been explored.For example, the pyramidal rule (Han et al., 2017;Zhang et al., 2017a) suggested to gradually increase the channels in all convolutions layer by layer, regardless of spatial size.Figure 1 visually summarizes these heuristics for setting channel numbers in a neural network.Beyond the macro-level heuristics across entire network, recent works (Sandler et al., 2018;He et al., 2016;Zhang et al., 2017a;Tan et al., 2018;Cai et al., 2018) have also digged into channel configuration for micro-level building blocks (a network building block is usually composed of  Han et al., 2017;Zhang et al., 2017a), and inside network building blocks ((a) -(f )) (Sandler et al., 2018;He et al., 2016;Han et al., 2017;Zhang et al., 2017a;Tan et al., 2018;Cai et al., 2018).several 1 × 1 and 3 × 3 convolutions).These micro-level heuristics have led to better speed-accuracy trade-offs.The first of its kind, bottleneck residual block, was introduced in ResNet (He et al., 2016).It is composed of 1 × 1, 3 × 3, and 1 × 1 convolutions, where the 1 × 1 layers are responsible for reducing and then restoring dimensions, leaving the 3 × 3 layer a bottleneck (4× reduction).MobileNet v2 (Sandler et al., 2018), however, argued that the bottleneck design is not efficient and proposed the inverted residual block where 1 × 1 layers are used for expanding feature first (6× expansion) and then projecting back after intermediate 3 × 3 depthwise convolution.Furthermore, MNasNet (Tan et al., 2018) and ProxylessNAS nets (Cai et al., 2018) included 3× expansion version of inverted residual block into search space, and achieved even better accuracy under similar runtime latency.Apart from these human-designed heuristics, efforts on automatically optimizing channel configuration have been made explicitly or implicitly.A recent work (Liu et al., 2018c) suggested that many network pruning methods (Liu et al., 2017b;Li et al., 2016;Luo et al., 2017;He et al., 2017;Huang & Wang, 2018;Han et al., 2015b) can be thought of as performing network architecture search for channel numbers.Liu et al. (Liu et al., 2018c) showed that training these pruned architectures from scratch leads to similar or even better performance than fine-tuning and pruning from a large model.More recently, MNasNet (Tan et al., 2018) proposed to directly search network architectures, including filter sizes, using reinforcement learning algorithms (Schulman et al., 2017;Heess et al., 2017).Although the search is performed on the factorized hierarchical search space, massive network samples and computational cost (Tan et al., 2018) are required for an optimized network architecture.In this work, we study how to set channel numbers in a neural network to achieve better accuracy under constrained resources.To start, the first and the most brute-force approach came in mind is the exhaustive search: training all possible channel configurations of a deep neural network for full epochs (e.g., MobileNets (Sandler et al., 2018;Howard et al., 2017) are trained for approximately 480 epochs on ImageNet).Then we can simply select the best performers that are qualified for efficiency constraints.However, it is undoubtedly impractical since the cost of this brute-force approach is too high.For example, we consider a 8-layer convolutional networks and a search space limited to 10 candidates of channel numbers (e.g., 32, 64, ..., 320) for each layer.As a result, there are totally 10 8 candidate network architectures.To address this challenge, we present a simple and one-shot solution AutoSlim.Our main idea lies in training a slimmable network (Yu et al., 2018) to approximate the network accuracy of different channel configurations.Yu et al. (Yu et al., 2018;Yu & Huang, 2019) introduced slimmable networks that can run at arbitrary width with equally or even better performance than same architecture trained individually.Although the original motivation is to provide instant and adaptive accuracyefficiency trade-offs, we find slimmable networks are especially suitable as benchmark performance estimators for several reasons: (1) Training slimmable models (using the sandwich rule (Yu & Huang, 2019)) is much faster than the brute-force approach.(2) A trained slimmable model can execute at arbitrary width, which can be used to approximate relative performance among different channel configurations.(3) The same trained slimmable model can be applied on search of optimal channels for different resource constraints.In AutoSlim, we first train a slimmable model for a few epochs (e.g., 10% to 20% of full training epochs) to quickly get a benchmark performance estimator.We then iteratively evaluate the trained slimmable model and greedily slim the layer with minimal accuracy drop on validation set (for ImageNet, we randomly hold out 50K samples of training set as validation set).After this single pass, we can obtain the optimized channel configurations under different resource constraints (e.g., network FLOPs limited to 150M, 300M and 600M).Finally we train these optimized architectures individually or jointly (as a single slimmable network) for full training epochs.We experiment with various networks including MobileNet v1, MobileNet v2, ResNet-50 and RL-searched MNasNet on the challenging setting of 1000-class ImageNet classification.AutoSlim achieves better results (with much lower search cost) compared with three baselines: (1) the default channel configuration of these networks, (2) channel pruning methods on same network architectures (Luo et al., 2017;He et al., 2017;Yang et al., 2018) and (3) reinforcement learning based architecture search methods (He et al., 2018;Tan et al., 2018).\n",
            "\n",
            "▶ Ground Truth 摘要：\n",
            " In the ever-evolving realm of neural networks, where every resource counts and efficiency reigns supreme, we embark on a quest to discover the optimal number of channels—those vital conduits of information—that can elevate accuracy while navigating the treacherous waters of constrained resources like FLOPs, latency, memory footprint, and model size. Enter AutoSlim, a novel and straightforward approach that defies the conventional wisdom of training countless network samples or relying on the whims of reinforcement learning. Instead, we forge a singular slimmable network, a master of adaptability, capable of approximating the accuracy across various channel configurations with remarkable precision.  Through a series of iterative evaluations, we wield this trained slimmable model, deftly trimming layers with the lightest touch—those that promise minimal sacrifice in accuracy. This elegant, one-pass strategy reveals the optimized channel configurations tailored to diverse resource constraints, proving both efficient and effective.  Our experiments unfold across renowned architectures: MobileNet v1, MobileNet v2, ResNet-50, and the RL-searched MNasNet, all tested against the formidable challenge of ImageNet classification. The results are nothing short of transformative, showcasing significant advancements over their default channel settings. Notably, our AutoSlim-MobileNet-v2, operating at 305M FLOPs, achieves a remarkable 74.2% top-1 accuracy—2.4% higher than its default counterpart and even outshining the RL-searched MNasNet at 317M FLOPs by 0.2%. Similarly, our AutoSlim-ResNet-50, at 570M FLOPs and devoid of depthwise convolutions, surpasses MobileNet-v1 by 1.3%.  In this journey, we not only enhance performance but do so with a staggering 100X reduction in search cost compared to contemporary channel pruning and neural architecture search methods. Thus, we carve a new path forward, one where precision and resourcefulness intertwine seamlessly in the pursuit of neural network excellence.\n",
            "\n",
            "▶ 模型生成摘要：\n",
            " In the realm of deep neural networks, the number of channels in a network plays a critical role in its affordability on resource constrained platforms, such as mobile phones, wearables, and Internet of Things (IoT) devices. In this paper, we introduce AutoSlim, a simple and one-shot solution that harnesses the power of a slimmable network to optimize channel configurations for different resource constraints. We first train a Slimmable model for a few epochs to quickly get a benchmark performance estimator. We then iteratively evaluate the trained SlimmABLE model and greedily slim the layer with minimal accuracy drop on validation set (for ImageNet, we randomly hold out 50K samples of training set as validation set). After this single pass, we can obtain the optimized channel configuration under different resources constraints (e.g., network FLOPs limited to 150M, 300M, and 600M). Finally, we train these optimized architectures individually or jointly (as a single slimmABLE network) for full training etochs. We experiment with various networks including MobileNet v1, MobileNet V2, ResNet-50, and RL-searched MNasNet on the challenging setting of 1000-class ImageNet classification. Our results demonstrate that Autoslim achieves better results (with much lower search cost) compared with three baselines: (1) the default channel configuration of these networks, (2) channel pruning methods on same network architectures, (3) reinforcement learning based architecture search methods.\n",
            "------------------------------------------------------\n",
            "\n",
            "------------------------------------------------------\n",
            "[Sample 2]\n",
            "\n",
            "▶ Prompt：\n",
            " You are a professional academic summarizer. Write a precise and objective abstract for the following research introduction. Do not include poetic or exaggerated language. Only describe the main objectives, methods, and key findings of the paper. If the text contains formulas, mathematical notations, or specific numerical results, retain them in the abstract. Do not add personal opinions or restate this prompt. Use a formal academic tone.\n",
            "\n",
            "Introduction: Kernel methods are among the most powerful machine learning approaches with a wide range of successful applications (Schölkopf & Smola, 2018) which, however, suffer from scalability issues in large-scale problems, due to their high space and time complexities (with respect to the number of data n).To address this key limitation, a myriad of random features based kernel approximation techniques have been proposed (Rahimi & Recht, 2008;Liu et al., 2021a): random features methods randomly project the data to obtain low-dimensional nonlinear representations that approximate the original kernel features.This allows practitioners to apply them, with a large saving in both time and space, to various kernel-based downstream tasks such as kernel spectral clustering (Von Luxburg, 2007), kernel principal component analysis (Schölkopf et al., 1997), kernel canonical correlation analysis (Lai & Fyfe, 2000), kernel ridge regression (Vovk, 2013), to name a few.A wide variety of these kernels can be written, for data points x i , x j ∈ R p , in the form κ(x i , x j ) = E w σ w T x i σ w T x j(1) with w ∈ R p having i.i.d.entries, which can be \"well approximated\" by a sample mean 1 m m t=1 σ w T t x i σ w T t x j over m independent random features for m sufficiently large.For instance, taking σ(x) = [cos(x), sin(x)] and w with i.i.d.standard Gaussian entries, one obtains the popular Random Fourier Features (RFFs) that approximate the Gaussian kernel (and the As shall be seen subsequently, (random) neural networks are, to a large extent, connected to kernel matrices of the form (1).More specifically, the classification or regression performance at the output of random neural networks are functionals of random matrices that fall into the wide class of kernel random matrices.Perhaps more surprisingly, this connection still exists for deep neural networks which are (i) randomly initialized and (ii) trained with gradient descent, as testified by the recent works on neural tangent kernels (Jacot et al., 2018), by considering the \"infinitely many neurons\" limit, that is, the limit where the network widths of all layers go to infinity simultaneously.This close connection between neural networks and kernels has triggered a renewed interest for the theoretical investigation of deep neural networks from various perspectives, including optimization (Du et al., 2019;Chizat et al., 2019), generalization (Allen- Zhu et al., 2018;Arora et al., 2019;Bietti & Mairal, 2019), and learning dynamics (Lee et al., 2019;Advani et al., 2020;Liao & Couillet, 2018a).These works shed new light on the theoretical understanding of deep neural network models and specifically demonstrate the significance of studying networks with random weights and their associated kernels to assess the mechanisms underlying more elaborate deep networks.In this article, we consider the random feature kernel of the type (1), which can also be seen as the limiting kernel of a single-hidden-layer neural network with a random first layer.By assuming a high-dimensional Gaussian Mixture Model (GMM) for the data {x i } n i=1 with x i ∈ R p , we show that the centered kernel matrixfoot_0 K P{κ(x i , x j )} n i,j=1 P, P I n -is asymptotically (as n, p → ∞ with p/n → c ∈ (0, ∞)) equivalent, in a spectral sense, to another random kernel matrix K which depends on the GMM data statistics and the generalized Gaussian moments E[σ (z)], E[σ (z)] of the activation function σ(•), but is independent of the specific law of the i.i.d.entries of the random vector w, as long as they are normalized to have zero mean and unit variance.As such, one can design novel random features schemes with limiting kernels asymptotically equivalent to the original K.For instance, define κ ter (x i , x j ) E w ter σ ter x T i w ter σ ter x T j w ter(3) with w ter ∈ R p having i.i.d.entries taking value w ter i = 0 (with probability ) and valueeach with probability 1/2 -/2, where ∈ [0, 1) represents the level of sparsity of w, andfor some thresholds s -< s + chosen to match the generalized Gaussian moments E[σ (z)], E[σ (z)] of any σ function (e.g., ReLU, cos, sin) widely used in random features or neural network contexts.The proposed Ternary Random Features (TRFs, with limiting kernel matrices defined in (3) asymptotically \"matching\" any random features kernel matrices in a spectral sense) have the computational advantage of being sparse and not requiring multiplications but only additions, as well as the storage advantage of being composed of only a finite set of words, e.g., {-1, 0, 1} for = 0.Given the urgent need for environmentally-friendly, but still efficient, neural networks such as binary neural networks (Hubara et al., 2016;Lin et al., 2015;Zhu et al., 2016;Qin et al., 2020;Hubara et al., 2016), pruned neural networks (Liu et al., 2015;Han et al., 2015a;b), weights-quantized neural networks (Gupta et al., 2015;Gong et al., 2014), we hope that our analysis will open a new door to a random matrix-improved framework of computationally efficient methods for machine learning and neural network models more generally.10 5 10 6 10 7 10 8 0.6 0.7 0.8 Memory (bits) Accuracy LP-RFF (8 bit) LP-RFF (1 bit) Nyström (32 bits) Nyström (16 bits) Proposed TRF Figure 1: Test accuracy of logistic regression on quantized random features for different number of features m ∈ {102 , 10 3 , 5.10 3 , 10 4 , 5.10 4 }, with LP-RFF (8-bit and 1-bit, in black) (Zhang et al., 2019), Nyström approximation (32 bits in red, 16 bits in green) (Williams & Seeger, 2001), versus the proposed TRF approach (in blue), on the two-class Cov-Type dataset from UCI ML repo, with n = 418 000 training samples, n test = 116 000 test samples, and data dimension p = 54.\n",
            "\n",
            "▶ Ground Truth 摘要：\n",
            " In this article, we embark on an exploration of the spectral behavior of kernel matrices derived from random features, specifically of the form   \\[ {\\bf K} = \\mathbb{E}_{{\\bf w}} \\left[\\sigma\\left({\\bf w}^{\\sf T}{\\bf x}_i\\right)\\sigma\\left({\\bf w}^{\\sf T}{\\bf x}_j\\right)\\right]_{i,j=1}^n. \\]  Here, \\(\\sigma(\\cdot)\\) is a nonlinear function, the data points \\({\\bf x}_1, \\ldots, {\\bf x}_n\\) lie within \\(\\mathbb{R}^p\\), and the random projection vector \\({\\bf w}\\) is characterized by independent and identically distributed (i.i.d.) entries. In a high-dimensional landscape where both the number of data points \\(n\\) and their dimensionality \\(p\\) swell to significant proportions, we unveil a remarkable truth: under a Gaussian mixture model for our data, the eigenspectrum of the kernel matrix \\({\\bf K}\\) becomes agnostic to the specific distribution of the i.i.d. entries of \\({\\bf w}\\) (provided they are zero-mean and unit-variance). Instead, it hinges solely on the function \\(\\sigma(\\cdot)\\) through its generalized Gaussian moments, specifically \\(\\mathbb{E}_{z\\sim \\mathcal N(0,1)}[\\sigma'(z)]\\) and \\(\\mathbb{E}_{z\\sim \\mathcal N(0,1)}[\\sigma''(z)]\\).  From this foundation, we introduce a groundbreaking approach we term Ternary Random Features (TRFs). This innovative technique (i) asymptotically aligns the limiting kernel with the original \\({\\bf K}\\) in a spectral context, and (ii) offers a computational and storage efficiency that is markedly superior. By deftly tuning both the function \\(\\sigma\\) and the random vector \\({\\bf w}\\) to take values in \\(\\{-1,0,1\\}\\) in a manner that is responsive to the data, we eliminate the need for multiplication in our computations. This leads to a dramatic reduction in storage requirements—specifically, a factor of \\(b\\) fewer bits compared to traditional random features, where \\(b\\) represents the bit count needed to store full precision values.  Moreover, our experiments with real-world data reveal that these substantial computational and storage efficiencies are not merely theoretical; they coincide with enhanced performance relative to leading methods in the realm of random features. Thus, the Ternary Random Features stand as a beacon of innovation, promising both elegance and efficacy in the complex landscape of high-dimensional data analysis.\n",
            "\n",
            "▶ 模型生成摘要：\n",
            " In the realm of machine learning, kernel methods are among the most powerful approaches with a wide range of successful applications. However, they suffer from scalability issues in large-scale problems, due to their high space and time complexities (with respect to the number of data n). To address this key limitation, a myriad of random features based kernel approximation techniques have been proposed (Rahimi & Recht, 2008;Liu et al., 2021a). In this paper, we introduce the concept of Ternary Random Features (TRFs), a novel approach that utilizes limiting kernel matrices asymptotically equivalent to the original kernel matrix (1). By assuming a high-dimensional Gaussian Mixture Model (GMM) for the data x i  n i=1 with x j  R p , we show that the centered kernel matrixfoot_0 K P(xi, xj)ni, P, P I n- is asympathetically equivalent, in a spectral sense, to another random kernel matrix K which depends on the GMM data statistics and the generalized Gausssian moments E[ (z)], E [(z)] of the activation function (•), but is independent of the specific law of the i.i.d.entries of the random vector w, as long as they are normalized to have zero mean and unit variance. As such, one can design novel random features schemes with limiting Kernel Matrixes (K) asymphatically \"matching\" any random features kernel matrixes, in both computational efficiency and storage efficiency. We demonstrate the efficacy of our approach on the two-class Cov-Type dataset from the UCI ML repo, with n = 418 000 training samples, n test = 116 000 test samples, and data dimension p = 54.\n",
            "------------------------------------------------------\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "metric_rouge = evaluate.load(\"rouge\", rouge_types=[\"rouge1\", \"rouge2\", \"rougeL\"])\n",
        "metric_bertscore = evaluate.load(\"bertscore\")\n",
        "\n",
        "ground_truths = references\n",
        "\n",
        "# 計算 ROUGE 和 BERTScore\n",
        "rouge = metric_rouge.compute(predictions=predictions, references=ground_truths, use_stemmer=True)\n",
        "bertscore = metric_bertscore.compute(predictions=predictions, references=ground_truths, lang=\"en\")\n",
        "\n",
        "print(\"=== 評估結果===\\n\")\n",
        "print(\"🔹 ROUGE Scores:\")\n",
        "print(f\"  ROUGE-1: {rouge['rouge1']:.4f}\")\n",
        "print(f\"  ROUGE-2: {rouge['rouge2']:.4f}\")\n",
        "print(f\"  ROUGE-L: {rouge['rougeL']:.4f}\")\n",
        "\n",
        "print(\"\\n🔹 BERTScore (Average):\")\n",
        "print(f\"  Precision:  {sum(bertscore['precision']) / len(bertscore['precision']):.4f}\")\n",
        "print(f\"  Recall:     {sum(bertscore['recall']) / len(bertscore['recall']):.4f}\")\n",
        "print(f\"  F1 Score:   {sum(bertscore['f1']) / len(bertscore['f1']):.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0304166a-9de0-439c-bb3f-9c2472e35f68",
        "id": "HHIyh-wJDdAG"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== 評估結果===\n",
            "\n",
            "🔹 ROUGE Scores:\n",
            "  ROUGE-1: 0.4381\n",
            "  ROUGE-2: 0.1284\n",
            "  ROUGE-L: 0.2112\n",
            "\n",
            "🔹 BERTScore (Average):\n",
            "  Precision:  0.8477\n",
            "  Recall:     0.8307\n",
            "  F1 Score:   0.8391\n"
          ]
        }
      ]
    }
  ]
}