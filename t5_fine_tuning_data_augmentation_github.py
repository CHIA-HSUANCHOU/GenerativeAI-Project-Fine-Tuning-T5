# -*- coding: utf-8 -*-
"""T5_fine_tuning_data_augmentation_github.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1asGJia6hX_5xE1Ow6hLk7_awnGeCbxQF

## å®‰è£
"""

!pip install transformers datasets evaluate scikit-learn

!pip install peft rouge_score bert_score

from datasets import Dataset, DatasetDict
from sklearn.model_selection import train_test_split
from transformers import (
    AutoTokenizer, AutoModelForSeq2SeqLM,
    T5ForConditionalGeneration, Seq2SeqTrainer, Seq2SeqTrainingArguments,
    DataCollatorForSeq2Seq, EarlyStoppingCallback
)
import random
import json
import gc
import torch
import os
import evaluate
import numpy as np

seed = 188
random.seed(seed)
np.random.seed(seed)
torch.manual_seed(seed)
torch.cuda.manual_seed_all(seed)

os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"

from google.colab import drive

drive.mount('/content/drive')

train_path = "/content/drive/MyDrive/data/train.json"
test_path = "/content/drive/MyDrive/data/test.json"

"""## T5 Full Fine Tuning with Data Augmentation"""

# === è¼‰å…¥ paraphrasing æ¨¡å‹ ===
paraphrase_model_id = "Vamsi/T5_Paraphrase_Paws"
paraphrase_tokenizer = AutoTokenizer.from_pretrained(paraphrase_model_id)
paraphrase_model = AutoModelForSeq2SeqLM.from_pretrained(paraphrase_model_id).to("cuda")

# === è¼‰å…¥åŸå§‹è³‡æ–™ ===
with open(train_path, "r", encoding="utf-8") as f:
    full_data = [json.loads(line) for line in f]

dataset = Dataset.from_list(full_data)

# === ç”¢ç”Ÿ paraphrased version ===
def paraphrase_batch(batch):
    inputs = ["paraphrase: " + text + " </s>" for text in batch["introduction"]]
    tokenized = paraphrase_tokenizer(inputs, return_tensors="pt", padding=True, truncation=True).to("cuda")
    outputs = paraphrase_model.generate(**tokenized, max_length=600, num_return_sequences=1, do_sample=True, top_k=120, top_p=0.95)
    decoded = paraphrase_tokenizer.batch_decode(outputs, skip_special_tokens=True)
    return {"augmented_introduction": decoded}

augmented_dataset = dataset.map(paraphrase_batch, batched=True, batch_size=8)

# === éš¨æ©Ÿé¸å– 50% paraphrased è³‡æ–™ï¼Œä¸¦ä¿ç•™åŸå§‹ introduction ===
random.seed(42)
random_augmented_data = []
for original, paraphrased in zip(full_data, augmented_dataset["augmented_introduction"]):
    if random.random() < 0.5:
        new_example = original.copy()
        new_example["original_intro"] = original["introduction"]
        new_example["introduction"] = paraphrased
        random_augmented_data.append(new_example)

print(f"éš¨æ©Ÿé¸å–çš„ paraphrased è³‡æ–™æ•¸é‡ï¼š{len(random_augmented_data)}")

# === BERTScore éæ¿¾éš¨æ©Ÿé¸å‡ºçš„ paraphrased çµæœ ===
from bert_score import score

metric_bertscore = evaluate.load("bertscore")
originals = [ex["original_intro"] for ex in random_augmented_data]
paraphrased = [ex["introduction"] for ex in random_augmented_data]

P, R, F1 = score(paraphrased, originals, lang="en", verbose=True)

# è¨­å®šé–€æª»
threshold = 0.85
bert_filtered_data = []
for ex, f1_score in zip(random_augmented_data, F1):
    if f1_score.item() > threshold:
        ex.pop("original_intro")
        bert_filtered_data.append(ex)

print(f"BERTScore > {threshold} çš„ paraphrased è³‡æ–™æ•¸é‡ï¼š{len(bert_filtered_data)}")

full_data_augmented = full_data + bert_filtered_data
print(f"full_data_augmentedçš„ç­†æ•¸ï¼š{len(full_data_augmented)}")

train_data, val_data = train_test_split(full_data_augmented, test_size=0.1, random_state=42)

dataset = DatasetDict({
    "train": Dataset.from_list(train_data),
    "validation": Dataset.from_list(val_data)
})

for i in range(3):
    print(f"\n[{i + 1}] paper_id: {dataset['train'][i]['paper_id']}")
    print("Introduction:", dataset['train'][i]['introduction'][:300], "...")
    print("Abstract:", dataset['train'][i]['abstract'])

print(f"train_dataçš„ç­†æ•¸ï¼š{len(train_data)}")

# === åƒæ•¸è¨­å®š ===
model_name = "google/flan-t5-base"

max_input_length = 2048
max_target_length = 600

# === è¼‰å…¥ tokenizer èˆ‡æ¨¡å‹ ===
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForSeq2SeqLM.from_pretrained(model_name)
model.gradient_checkpointing_enable()

# === é è™•ç†å‡½å¼ ===
def preprocess(example):
    prompt = (
    "You are a professional academic summarizer. "
    "Write a precise and objective abstract for the following research introduction. "
    "Do not include poetic or exaggerated language. "
    "Only describe the main objectives, methods, and key findings of the paper. "
    "If the text contains formulas, mathematical notations, or specific numerical results, retain them in the abstract. "
    "Do not add personal opinions or restate this prompt. Use a formal academic tone.\n\n"
    "Introduction: "
    )
    inputs = prompt + example["introduction"]
    targets = example["abstract"]
    model_inputs = tokenizer(inputs, max_length=max_input_length, truncation=True, padding="max_length")
    with tokenizer.as_target_tokenizer():
        labels = tokenizer(targets, max_length=max_target_length, truncation=True, padding="max_length")["input_ids"]
        labels = [t if t != tokenizer.pad_token_id else -100 for t in labels]
    model_inputs["labels"] = labels
    return model_inputs

tokenized_dataset = dataset.map(preprocess, batched=False)

print(tokenizer.decode(tokenized_dataset["train"][0]["input_ids"], skip_special_tokens=True))

from functools import reduce
from operator import mul

total_params = sum(p.numel() for p in model.parameters())
total_size = sum(p.numel() * p.element_size() for p in model.parameters()) / (1024 ** 2)

print(f"ğŸ§  Total parameters: {total_params:,}")
print(f"ğŸ’¾ Approx. model size: {total_size:.2f} MB")

# === è¨“ç·´åƒæ•¸è¨­å®š ===
# === è¨“ç·´åƒæ•¸è¨­å®šï¼ˆå·²å„ªåŒ–è¨˜æ†¶é«”ï¼‰ ===
training_args = Seq2SeqTrainingArguments(
    output_dir="./t5_17",
    eval_strategy="epoch",
    save_strategy="epoch",
    logging_strategy="steps",
    logging_steps=10,
    per_device_train_batch_size=1,
    per_device_eval_batch_size=1,
    learning_rate=2.5e-5,
    num_train_epochs=20,
    predict_with_generate=False,
    fp16=False,
    bf16=True,
    save_total_limit=2,
    load_best_model_at_end=True,
    metric_for_best_model="eval_loss",
    label_smoothing_factor = 0.1,
    warmup_steps=500,
    lr_scheduler_type="linear",
    greater_is_better=False,
    report_to="none",
    seed = seed
)


# === å»ºç«‹ Data Collatorï¼ˆé¿å… loss ç‚º nanï¼‰ ===
data_collator = DataCollatorForSeq2Seq(
    tokenizer=tokenizer,
    model=model,
    label_pad_token_id=-100,
    padding=True
)

# === é–‹å§‹è¨“ç·´ ===
trainer = Seq2SeqTrainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_dataset["train"],
    eval_dataset=tokenized_dataset["validation"],
    tokenizer=tokenizer,
    data_collator=data_collator
)
trainer.train()

# å„²å­˜æœ¬åœ°å‚™ä»½
trainer.save_model("t5_17_final")
tokenizer.save_pretrained("t5_17_final")

# === ä¸Šå‚³æ¨¡å‹åˆ° Hugging Face Hub ===
#from huggingface_hub import login
#login("hf_xxxxxxx")

#model.push_to_hub("xxx/xxxlora171")
#tokenizer.push_to_hub("xxx/xxxlora171")

del model
del tokenizer
gc.collect()
torch.cuda.empty_cache()

"""### é©—è­‰"""

#from transformers import AutoTokenizer, AutoModelForSeq2SeqLM

#model = AutoModelForSeq2SeqLM.from_pretrained("xxx/xxxlora171")
#tokenizer = AutoTokenizer.from_pretrained("xxx/xxxlora171")

model = AutoModelForSeq2SeqLM.from_pretrained("/content/t5_17_final")
tokenizer = AutoTokenizer.from_pretrained("/content/t5_17_final")

from functools import reduce
from operator import mul

total_params = sum(p.numel() for p in model.parameters())
total_size = sum(p.numel() * p.element_size() for p in model.parameters()) / (1024 ** 2)

print(f"Total parameters: {total_params:,}")
print(f"Approx. model size: {total_size:.2f} MB")

# === æ¨ç†å‡½å¼ ===
def generate_summary(text):
    prompt = (
    "You are a professional academic summarizer. "
    "Write a precise and objective abstract for the following research introduction. "
    "Do not include poetic or exaggerated language. "
    "Only describe the main objectives, methods, and key findings of the paper. "
    "If the text contains formulas, mathematical notations, or specific numerical results, retain them in the abstract. "
    "Do not add personal opinions or restate this prompt. Use a formal academic tone.\n\n"
    "Introduction: "
    )

    input_text = prompt + text
    inputs = tokenizer(input_text, return_tensors="pt", truncation=True, max_length=max_input_length).to(model.device)
    outputs = model.generate(
    **inputs,
    max_new_tokens=max_target_length,
    min_length=200,
    num_beams=4,
    early_stopping=True,
    repetition_penalty=1.2,
    no_repeat_ngram_size=3,
)
    return tokenizer.decode(outputs[0], skip_special_tokens=True)

from tqdm import tqdm

print("\n=== é©—è­‰é›†æ‘˜è¦é æ¸¬ï¼ˆå‰ 3 ç­†ï¼‰ ===\n")
predictions = []
references = []
prompts = []

for sample in tqdm(dataset["validation"], desc="Generating summaries"):
    article = sample["introduction"]
    gt_abstract = sample.get("abstract", "")

    prompt = (
    "You are a professional academic summarizer. "
    "Write a precise and objective abstract for the following research introduction. "
    "Do not include poetic or exaggerated language. "
    "Only describe the main objectives, methods, and key findings of the paper. "
    "If the text contains formulas, mathematical notations, or specific numerical results, retain them in the abstract. "
    "Do not add personal opinions or restate this prompt. Use a formal academic tone.\n\n"
    "Introduction: " + article
    )

    summary = generate_summary(article)
    predictions.append(summary.strip())
    prompts.append(prompt.strip())
    references.append(gt_abstract.strip())

for i in range(3):
    print("------------------------------------------------------")
    print(f"[Sample {i + 1}]")
    print("\nâ–¶ Promptï¼š\n", prompts[i])
    print("\nâ–¶ Ground Truth æ‘˜è¦ï¼š\n", references[i])
    print("\nâ–¶ æ¨¡å‹ç”Ÿæˆæ‘˜è¦ï¼š\n", predictions[i])
    print("------------------------------------------------------\n")

metric_rouge = evaluate.load("rouge", rouge_types=["rouge1", "rouge2", "rougeL"])
metric_bertscore = evaluate.load("bertscore")
ground_truths = references
rouge = metric_rouge.compute(predictions=predictions, references=ground_truths, use_stemmer=True)
bertscore = metric_bertscore.compute(predictions=predictions, references=ground_truths, lang="en")
print("=== è©•ä¼°çµæœ===\n")
print("ğŸ”¹ ROUGE Scores:")
print(f"  ROUGE-1: {rouge['rouge1']:.4f}")
print(f"  ROUGE-2: {rouge['rouge2']:.4f}")
print(f"  ROUGE-L: {rouge['rougeL']:.4f}")

print("\nğŸ”¹ BERTScore (Average):")
print(f"  Precision:  {sum(bertscore['precision']) / len(bertscore['precision']):.4f}")
print(f"  Recall:     {sum(bertscore['recall']) / len(bertscore['recall']):.4f}")
print(f"  F1 Score:   {sum(bertscore['f1']) / len(bertscore['f1']):.4f}")